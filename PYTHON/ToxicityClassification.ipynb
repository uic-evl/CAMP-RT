{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /media/andrew/DATA/git_repos/CAMP-RT/PYTHON/preprocessing.py:11: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "from PatientSet import *\n",
    "from Constants import Constants\n",
    "import Metrics\n",
    "from analysis import *\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "from Classifiers import *\n",
    "from sklearn.preprocessing import OneHotEncoder, QuantileTransformer\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn import under_sampling, over_sampling, combine\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "baseline_feature_file = Constants.baseline_feature_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def presplit_roc_cv(classifier, data_split):\n",
    "    ypred = np.zeros((len(data_split),))\n",
    "    y = np.array([split['ytest'] for split in data_split])\n",
    "    i = 0\n",
    "    for split in data_split:\n",
    "        classifier.fit(split['xtrain'], split['ytrain'])\n",
    "        ypred[i] = classifier.predict_proba(split['xtest'])[0,1]\n",
    "        if i == 0:\n",
    "            has_importances = hasattr(classifier, 'feature_importances_')\n",
    "        if has_importances:\n",
    "            if i == 0:\n",
    "                importances = classifier.feature_importances_\n",
    "            else:\n",
    "                importances += classifier.feature_importances_\n",
    "        i += 1\n",
    "    if has_importances:\n",
    "        importances /= i\n",
    "        importances = pd.Series(data = importances, index = data_split[0]['feature_labels'])\n",
    "    else:\n",
    "        importances = None\n",
    "    return roc_auc_score(y, ypred), importances\n",
    "\n",
    "def discretize_continuous_fields(df, n_bins):\n",
    "    encoder = KBinsDiscretizer(n_bins = n_bins, encode = 'ordinal')\n",
    "    for col in df.columns:\n",
    "        vals = df[col].values\n",
    "        if len(np.unique(vals)) > n_bins:\n",
    "            df[col] = encoder.fit_transform(vals.reshape(-1,1)).ravel()\n",
    "    return df\n",
    "\n",
    "def get_all_splits(df, regularizer, outcomes):\n",
    "    data_splits = {}\n",
    "    for outcome in outcomes:\n",
    "        splits = {str(resampler): get_splits(df, outcome[0], regularizer, [resampler]) for resampler in resamplers}\n",
    "        data_splits[outcome[1]] = splits\n",
    "    return data_splits\n",
    "\n",
    "def get_splits(df, y, regularizer = None, resamplers = None):\n",
    "    x = df.values\n",
    "    feature_labels = list(df.columns)\n",
    "    loo = LeaveOneOut()\n",
    "    splits = []\n",
    "    for train, test in loo.split(x):\n",
    "        split = {}\n",
    "        xtrain, ytrain = x[train], y[train]\n",
    "        xtest, ytest = x[test], y[test]\n",
    "        if regularizer is not None:\n",
    "            xtrain = regularizer.fit_transform(xtrain)\n",
    "            xtest = regularizer.transform(xtest)\n",
    "        for resampler in resamplers:\n",
    "            if resampler is None:\n",
    "                continue\n",
    "            xtrain, ytrain = resampler.fit_resample(xtrain, ytrain)\n",
    "        split['xtrain'] = xtrain\n",
    "        split['xtest'] = xtest\n",
    "        split['ytrain'] = ytrain\n",
    "        split['ytest'] = ytest\n",
    "        split['train_index'] = train\n",
    "        split['test_index'] = test\n",
    "        split['feature_labels'] = feature_labels\n",
    "        splits.append(split)\n",
    "    return splits\n",
    "\n",
    "def cluster_features(db,\n",
    "                  baseline_features = baseline_feature_file,\n",
    "                  use_baseline_features = True,\n",
    "                  top_features = 'data/clustering_results/toxicityClustering.csv',\n",
    "                  use_top_features = True,\n",
    "                  discrete_features = False,\n",
    "                  cluster_names = ['kmeans_k=4','cluster_labels']):\n",
    "    baseline = pd.read_csv(baseline_features, index_col = 'Dummy.ID').drop('Unnamed: 0', axis = 1)\n",
    "    all_clusters = set(['manhattan_k=2','manhattan_k=3','manhattan_k=4',\n",
    "                        'cluster_labels','hc_ward2','hc_ward4',\n",
    "                        'FT','AR','TOX'])\n",
    "    non_features = list(all_clusters - set(cluster_names))\n",
    "    \n",
    "    if use_baseline_features:\n",
    "        cluster_names = cluster_names + list(baseline.drop(non_features, axis = 1, errors='ignore').columns)\n",
    "    if 'T.category' in cluster_names:\n",
    "        dist_clusters['T.category'] = dist_clusters['T.category'].apply(lambda x: int(x[1]))\n",
    "        \n",
    "    if isinstance(top_features, str):\n",
    "        dist_clusters = pd.read_csv(top_features, index_col = 0)\n",
    "        dist_clusters.index.rename('Dummy.ID', inplace = True)\n",
    "        if use_top_features:\n",
    "            cluster_names = cluster_names + list( dist_clusters.drop(non_features,axis=1, errors='ignore').columns)\n",
    "        df = baseline.merge(dist_clusters, on=['Dummy.ID'])\n",
    "    else:\n",
    "        df = baseline\n",
    "    ft = df.FT.values\n",
    "    ar = df.AR.values\n",
    "    tox = df.TOX.values\n",
    "    to_drop = set(df.columns) - set(cluster_names)\n",
    "    df = df.drop(to_drop, axis = 1, errors = 'ignore')\n",
    "    if discrete_features:\n",
    "        df = discretize_continuous_fields(df, 5)\n",
    "    columns = df.columns\n",
    "    for col in columns:\n",
    "        if col in all_clusters:\n",
    "            groups = set(df[col].values)\n",
    "            for g in groups:\n",
    "                col_name = col + '=' + str(g)\n",
    "                df[col_name] = df[col].values == g\n",
    "            df = df.drop(col, axis = 1)\n",
    "    return df, ft, ar, tox\n",
    "\n",
    "def test_classifiers(classifiers, \n",
    "                     db = None, \n",
    "                     log = False,\n",
    "                     feature_params = {},\n",
    "                     regularizer = QuantileTransformer(),\n",
    "                     data_splits = None,\n",
    "                     print_importances = False,\n",
    "                    additional_features = None):\n",
    "\n",
    "    result_template = {'cluster_names': copy(str(feature_params['cluster_names'])),\n",
    "                       'Baseline': str(feature_params['use_baseline_features']),\n",
    "                       'Top_features': str(feature_params['use_top_features']),\n",
    "                       'Top_feature_file': str(feature_params['top_features']),\n",
    "                      }\n",
    "\n",
    "    if log:\n",
    "        timestamp = datetime.fromtimestamp(time()).strftime('%Y_%m_%d_%H%M%S')\n",
    "        f = open(Constants.toxicity_log_file_root + timestamp +'.txt', 'w', buffering = 1)\n",
    "        def write(string):\n",
    "            print(string)\n",
    "            f.write(str(string)+'\\n')\n",
    "    else:\n",
    "        write = lambda string: print(string)\n",
    "        \n",
    "    df, ft, ar, tox = cluster_features(db, **feature_params)\n",
    "    if additional_features is not None:\n",
    "        #should be tuple of attributes, organ_list (default none) to pass to patientset.to_dataframe\n",
    "        df = db.to_dataframe(additional_features[0], df, additional_features[1])\n",
    "    write(str(feature_params))\n",
    "    outcomes = [(ft, 'feeding_tube'), (ar, 'aspiration'), (tox, 'toxicity')]\n",
    "    data_splits = get_all_splits(df, regularizer, outcomes) if data_splits is None else data_splits\n",
    "    print('splits finished')\n",
    "    results = []\n",
    "    for classifier in classifiers:\n",
    "        write(classifier)\n",
    "        for outcome in outcomes:\n",
    "            data_split = data_splits[outcome[1]]\n",
    "            for resampler_name, splits in data_split.items():\n",
    "                try:\n",
    "                    write(resampler_name)\n",
    "                    auc, importances = presplit_roc_cv(classifier, splits)\n",
    "                    write(outcome[1])\n",
    "                    write(auc)\n",
    "                    if importances is not None and print_importances:\n",
    "                        write(importances)\n",
    "                    write('\\n')\n",
    "                    result = copy(result_template)\n",
    "                    result['classifier'] = str(classifier)\n",
    "                    result['outcome'] = str(outcome[1])\n",
    "                    result['resampler'] = str(resampler_name)\n",
    "                    result['AUC'] = auc\n",
    "                    results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "    if log:\n",
    "        f.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = load_patientset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_db(db = None, db_args = {}):\n",
    "    if db is None:\n",
    "        db = PatientSet(**db_args)\n",
    "    db.toxicity = db.feeding_tubes + db.aspiration > 0\n",
    "    return(db)\n",
    "classifiers = [\n",
    "#                    DecisionTreeClassifier(),\n",
    "#                    DecisionTreeClassifier(criterion='entropy'),\n",
    "#                    XGBClassifier(1, booster = 'gblinear'),\n",
    "#                    XGBClassifier(3, booster = 'gblinear'),\n",
    "#                    XGBClassifier(5, booster = 'gblinear'),\n",
    "#                    XGBClassifier(),\n",
    "#                    XGBClassifier(booster = 'dart'),\n",
    "                    LogisticRegression(solver = 'lbfgs', max_iter = 8000),\n",
    "#                    MetricLearningClassifier(use_softmax = True),\n",
    "#                    MetricLearningClassifier(\n",
    "#                            resampler = under_sampling.OneSidedSelection()),\n",
    "#                    MetricLearningClassifier(\n",
    "#                            resampler = under_sampling.CondensedNearestNeighbour()),\n",
    "#                    ExtraTreesClassifier(n_estimators = 200),\n",
    "#                    RandomForestClassifier(n_estimators = 200, max_depth = 3),\n",
    "#                    BayesWrapper(),\n",
    "                   ]\n",
    "\n",
    "resamplers = [\n",
    "                  None,\n",
    "#                  under_sampling.RandomUnderSampler(),\n",
    "#                  over_sampling.RandomOverSampler(),\n",
    "#                  under_sampling.InstanceHardnessThreshold(\n",
    "#                          estimator = MetricLearningClassifier(),\n",
    "#                          cv = 18),\n",
    "#                  under_sampling.InstanceHardnessThreshold(cv = 18),\n",
    "#                  over_sampling.SMOTE(),\n",
    "#                  combine.SMOTEENN(),\n",
    "#                  combine.SMOTETomek(),\n",
    "#                  under_sampling.InstanceHardnessThreshold(),\n",
    "#                  under_sampling.RepeatedEditedNearestNeighbours(),\n",
    "#                  under_sampling.EditedNearestNeighbours(),\n",
    "#                  under_sampling.CondensedNearestNeighbour(),\n",
    "#                  under_sampling.OneSidedSelection(),\n",
    "                  ]\n",
    "\n",
    "cluster_root= 'data/clustering_results/'\n",
    "feature_file_names = ['metaClusteringBootstrapped500MinmaxBest']\n",
    "#llop through all files of feature extracted from toxicity clustering\n",
    "all_results = []\n",
    "additional_features = None\n",
    "\n",
    "run = lambda x: test_classifiers(classifiers, db, \n",
    "                                 log = True, \n",
    "                                 feature_params = x, \n",
    "                                 additional_features = additional_features)\n",
    "do_test = lambda: all_results.extend(run(feature_params))\n",
    "\n",
    "def try_features():\n",
    "    for feature_file in feature_file_names:\n",
    "        feature_params['top_features'] = cluster_root + feature_file + '.csv'\n",
    "        do_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline\n",
      "feeding_tubes\n",
      "0.7258682328907048\n",
      "aspiration\n",
      "0.7525443442861297\n",
      "toxicity\n",
      "0.7402551381998583\n",
      "\n",
      "metaClusteringBootstrapped500MinmaxBest\n",
      "feeding_tubes\n",
      "0.7709397344228806\n",
      "aspiration\n",
      "0.7685373655132305\n",
      "toxicity\n",
      "0.8003189227498229\n",
      "\n",
      "metaClusteringBootstrapped500MinmaxBest+baseline\n",
      "feeding_tubes\n",
      "0.7935393258426966\n",
      "aspiration\n",
      "0.8242221576039547\n",
      "toxicity\n",
      "0.8251240255138199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#the the unsupervised AUC scores\n",
    "def print_cluster_results(clusters, name):\n",
    "    print(name)\n",
    "    for outcome in ['feeding_tubes', 'aspiration','toxicity']:\n",
    "        print(outcome)\n",
    "        tox = getattr(db, outcome)\n",
    "        pred_outcome = BayesWrapper().fit_predict(clusters, tox.reshape(-1,1), True)[:,1]\n",
    "        print(roc_auc_score(tox, pred_outcome))\n",
    "    print()\n",
    "    \n",
    "baseline_clusters = pd.read_csv(baseline_feature_file, index_col='Dummy.ID')['manhattan_k=4'].values.reshape(-1,1)\n",
    "print_cluster_results(baseline_clusters, 'baseline')\n",
    "\n",
    "for feature_file in feature_file_names:\n",
    "    file = cluster_root + feature_file + '.csv'\n",
    "    spatial_clusters = pd.read_csv(file).cluster_labels.values.reshape(-1,1)\n",
    "    print_cluster_results(spatial_clusters, feature_file)\n",
    "    both_clusters = np.hstack([spatial_clusters, baseline_clusters])\n",
    "    print_cluster_results(both_clusters, feature_file + '+baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'use_baseline_features': False, 'top_features': None, 'use_top_features': False, 'discrete_features': False, 'cluster_names': ['manhattan_k=4']}\n",
      "splits finished\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=8000,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "None\n",
      "feeding_tube\n",
      "0.586567926455567\n",
      "\n",
      "\n",
      "None\n",
      "aspiration\n",
      "0.49723756906077343\n",
      "\n",
      "\n",
      "None\n",
      "toxicity\n",
      "0.6031183557760453\n",
      "\n",
      "\n",
      "{'use_baseline_features': False, 'top_features': 'data/clustering_results/metaClusteringBootstrapped500MinmaxBest.csv', 'use_top_features': False, 'discrete_features': False, 'cluster_names': ['manhattan_k=4', 'cluster_labels']}\n",
      "splits finished\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=8000,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "None\n",
      "feeding_tube\n",
      "0.7216547497446375\n",
      "\n",
      "\n",
      "None\n",
      "aspiration\n",
      "0.7380052340796744\n",
      "\n",
      "\n",
      "None\n",
      "toxicity\n",
      "0.7347625797306874\n",
      "\n",
      "\n",
      "{'use_baseline_features': False, 'top_features': 'data/clustering_results/metaClusteringBootstrapped500MinmaxBest.csv', 'use_top_features': False, 'discrete_features': False, 'cluster_names': ['cluster_labels']}\n",
      "splits finished\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=8000,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "None\n",
      "feeding_tube\n",
      "0.6281920326864148\n",
      "\n",
      "\n",
      "None\n",
      "aspiration\n",
      "0.6420471067170689\n",
      "\n",
      "\n",
      "None\n",
      "toxicity\n",
      "0.6856839121190644\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#try out combinations of just clusters\n",
    "feature_params = {\n",
    "    'use_baseline_features': False,\n",
    "    'top_features': None,\n",
    "    'use_top_features': False,       \n",
    "    'discrete_features': False,\n",
    "    'cluster_names': ['manhattan_k=4']\n",
    "}\n",
    "do_test()\n",
    "for cluster_combo in [['manhattan_k=4', 'cluster_labels'],['cluster_labels']]:\n",
    "    feature_params['cluster_names'] = cluster_combo\n",
    "    try_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'use_baseline_features': True, 'top_features': 'data/clustering_results/metaClusteringBootstrapped500MinmaxBest.csv', 'use_top_features': False, 'discrete_features': False, 'cluster_names': []}\n",
      "splits finished\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=8000,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "None\n",
      "feeding_tube\n",
      "0.6404494382022471\n",
      "\n",
      "\n",
      "None\n",
      "aspiration\n",
      "0.8493748182611224\n",
      "\n",
      "\n",
      "None\n",
      "toxicity\n",
      "0.7888022678951098\n",
      "\n",
      "\n",
      "{'use_baseline_features': True, 'top_features': 'data/clustering_results/metaClusteringBootstrapped500MinmaxBest.csv', 'use_top_features': False, 'discrete_features': False, 'cluster_names': ['manhattan_k=4']}\n",
      "splits finished\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=8000,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "None\n",
      "feeding_tube\n",
      "0.6350868232890705\n",
      "\n",
      "\n",
      "None\n",
      "aspiration\n",
      "0.8470485606280895\n",
      "\n",
      "\n",
      "None\n",
      "toxicity\n",
      "0.7854358610914246\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#baseline test\n",
    "feature_params['use_baseline_features'] = True\n",
    "for cluster_combo in [[],['manhattan_k=4']]:\n",
    "    feature_params['cluster_names'] = cluster_combo\n",
    "    do_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'use_baseline_features': True, 'top_features': 'data/clustering_results/metaClusteringBootstrapped500MinmaxBest.csv', 'use_top_features': False, 'discrete_features': False, 'cluster_names': ['manhattan_k=4', 'cluster_labels']}\n",
      "splits finished\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=8000,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "None\n",
      "feeding_tube\n",
      "0.6856486210418795\n",
      "\n",
      "\n",
      "None\n",
      "aspiration\n",
      "0.8450130851991857\n",
      "\n",
      "\n",
      "None\n",
      "toxicity\n",
      "0.8261871013465627\n",
      "\n",
      "\n",
      "{'use_baseline_features': True, 'top_features': 'data/clustering_results/metaClusteringBootstrapped500MinmaxBest.csv', 'use_top_features': False, 'discrete_features': False, 'cluster_names': ['cluster_labels']}\n",
      "splits finished\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=8000,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "None\n",
      "feeding_tube\n",
      "0.6935648621041879\n",
      "\n",
      "\n",
      "None\n",
      "aspiration\n",
      "0.8473393428322187\n",
      "\n",
      "\n",
      "None\n",
      "toxicity\n",
      "0.8307937632884478\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test out adding clusters from each type (baseline + clusters)\n",
    "feature_params['use_baseline_features'] = True\n",
    "feature_params['use_top_features'] = False\n",
    "feature_params['cluster_names'] = ['manhattan_k=4', 'cluster_labels']\n",
    "try_features()\n",
    "\n",
    "feature_params['cluster_names'] = ['cluster_labels']\n",
    "try_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'use_baseline_features': False, 'top_features': 'data/clustering_results/metaClusteringBootstrapped500MinmaxBest.csv', 'use_top_features': True, 'discrete_features': False, 'cluster_names': ['cluster_labels']}\n",
      "splits finished\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=8000,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "None\n",
      "feeding_tube\n",
      "0.6945863125638406\n",
      "\n",
      "\n",
      "None\n",
      "aspiration\n",
      "0.7760977028205874\n",
      "\n",
      "\n",
      "None\n",
      "toxicity\n",
      "0.7414953933380581\n",
      "\n",
      "\n",
      "{'use_baseline_features': True, 'top_features': 'data/clustering_results/metaClusteringBootstrapped500MinmaxBest.csv', 'use_top_features': True, 'discrete_features': False, 'cluster_names': ['cluster_labels']}\n",
      "splits finished\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=8000,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "None\n",
      "feeding_tube\n",
      "0.6754341164453525\n",
      "\n",
      "\n",
      "None\n",
      "aspiration\n",
      "0.8377435300959581\n",
      "\n",
      "\n",
      "None\n",
      "toxicity\n",
      "0.8182140326009921\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#now try just spatial features\n",
    "feature_params['use_top_features'] = True\n",
    "feature_params['use_baseline_features'] = False\n",
    "feature_params['cluster_names']= ['cluster_labels']\n",
    "try_features()\n",
    "\n",
    "feature_params['use_baseline_features'] = True\n",
    "try_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save it all\n",
    "df = pd.DataFrame(all_results).sort_values(\n",
    "        ['classifier',\n",
    "         'outcome',\n",
    "         'AUC',\n",
    "         'resampler',\n",
    "         'cluster_names',\n",
    "         'Baseline'],\n",
    "         kind = 'mergesort',\n",
    "         ascending = False)\n",
    "df.to_csv('data/toxcity_classification_tests_'\n",
    "          + datetime.fromtimestamp(time()).strftime('%Y_%m_%d_%H%M%S')\n",
    "          + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
